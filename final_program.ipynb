{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMfn44vvW21Ma4Nf4DhaIfX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdkcJ7bVkBPw","executionInfo":{"status":"ok","timestamp":1702133069810,"user_tz":-540,"elapsed":16880,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"c2700767-9d1e-4e5c-e8bf-d956debf1468"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["#라이브러리"],"metadata":{"id":"c8hobPkz7gtv"}},{"cell_type":"code","source":["## install\n","!pip install git+https://github.com/openai/whisper.git"],"metadata":{"id":"T6CqFwwL7gQY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702133096434,"user_tz":-540,"elapsed":26626,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"cf95b378-2707-4d69-eb6b-30324c1a1c23"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-m_kyfgb2\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-m_kyfgb2\n","  Resolved https://github.com/openai/whisper.git to commit e58f28804528831904c3b6f2c0e473f346223433\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n","Collecting tiktoken (from openai-whisper==20231117)\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n","Building wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=26842e5f3d34cb252cc17e397a26b8a90e592ffd1373447cd4bdc28ec8f75a05\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-81z6a7rr/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n","Successfully built openai-whisper\n","Installing collected packages: tiktoken, openai-whisper\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n"]}]},{"cell_type":"code","source":["!pip install ffmpeg-python"],"metadata":{"id":"dwELZVD_7i3M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702133105364,"user_tz":-540,"elapsed":8936,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"87df9e78-11b4-4d8e-c4eb-0068d2a67b4d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ffmpeg-python\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python) (0.18.3)\n","Installing collected packages: ffmpeg-python\n","Successfully installed ffmpeg-python-0.2.0\n"]}]},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43wYRChVFeKj","executionInfo":{"status":"ok","timestamp":1702133119428,"user_tz":-540,"elapsed":14076,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"49fd2247-34e1-40a6-8254-0f20ce845b31"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["import whisper\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import librosa\n","import librosa.display\n","from pprint import pprint\n","from IPython.display import HTML, Audio\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","from scipy.io.wavfile import read as wav_read\n","import io\n","import ffmpeg\n","import tempfile\n","import wave\n","import subprocess\n","\n","from urllib.error import HTTPError\n","from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import json\n","from konlpy.tag import Okt\n","\n","import re\n","import urllib.request\n","import time\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","\n","okt = Okt()\n","path = '/content/gdrive/MyDrive/4학년 2학기/창종설/data/'\n","\n","df_common_term = pd.read_csv(path + '/csv/common_term.csv')\n","\n","model_whisper = whisper.load_model(\"medium\")"],"metadata":{"id":"ySKaun2U7fsj","executionInfo":{"status":"ok","timestamp":1702133190533,"user_tz":-540,"elapsed":71111,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"62cf65cf-85cc-4117-fa4b-a0d53b441f3a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|█████████████████████████████████████| 1.42G/1.42G [00:16<00:00, 93.1MiB/s]\n"]}]},{"cell_type":"markdown","source":["# whisper"],"metadata":{"id":"ztZXOPTekic4"}},{"cell_type":"code","source":["stop_words = ['당한', '것']\n","\n","def stop_words_func(text_list):\n","  text_stopwords = []\n","  for text in text_list:\n","      word_tokens = okt.morphs(text)\n","      if not text in stop_words:\n","        text_stopwords.append(text)\n","      # # 불용어가 제거된 형태소 분석 결과 출력\n","      # result = ' '.join(text_stopwords)\n","      # text_stopwords.append(result)\n","\n","  return text_stopwords"],"metadata":{"id":"rvojyJSdFkt-","executionInfo":{"status":"ok","timestamp":1702133912146,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def whisper_stt():\n","\n","  def get_audio():\n","\n","    AUDIO_HTML = \"\"\"\n","    <script>\n","    var my_div = document.createElement(\"DIV\");\n","    var my_p = document.createElement(\"P\");\n","    var my_btn = document.createElement(\"BUTTON\");\n","    var t = document.createTextNode(\"Press to start recording\");\n","\n","    my_btn.appendChild(t);\n","    //my_p.appendChild(my_btn);\n","    my_div.appendChild(my_btn);\n","    document.body.appendChild(my_div);\n","\n","    var base64data = 0;\n","    var reader;\n","    var recorder, gumStream;\n","    var recordButton = my_btn;\n","\n","    var handleSuccess = function(stream) {\n","      gumStream = stream;\n","      var options = {\n","        //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n","        mimeType : 'audio/webm;codecs=opus'\n","        //mimeType : 'audio/webm;codecs=pcm'\n","      };\n","      //recorder = new MediaRecorder(stream, options);\n","      recorder = new MediaRecorder(stream);\n","      recorder.ondataavailable = function(e) {\n","        var url = URL.createObjectURL(e.data);\n","        var preview = document.createElement('audio');\n","        preview.controls = true;\n","        preview.src = url;\n","        document.body.appendChild(preview);\n","\n","        reader = new FileReader();\n","        reader.readAsDataURL(e.data);\n","        reader.onloadend = function() {\n","          base64data = reader.result;\n","          //console.log(\"Inside FileReader:\" + base64data);\n","        }\n","      };\n","      recorder.start();\n","      };\n","\n","    recordButton.innerText = \"Recording... press to stop\";\n","\n","    navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n","\n","\n","    function toggleRecording() {\n","      if (recorder && recorder.state == \"recording\") {\n","          recorder.stop();\n","          gumStream.getAudioTracks()[0].stop();\n","          recordButton.innerText = \"Saving the recording... pls wait!\"\n","      }\n","    }\n","\n","    // https://stackoverflow.com/a/951057\n","    function sleep(ms) {\n","      return new Promise(resolve => setTimeout(resolve, ms));\n","    }\n","\n","    var data = new Promise(resolve=>{\n","    //recordButton.addEventListener(\"click\", toggleRecording);\n","    recordButton.onclick = ()=>{\n","    toggleRecording()\n","\n","    sleep(2000).then(() => {\n","      // wait 2000ms for the data to be available...\n","      // ideally this should use something like await...\n","      //console.log(\"Inside data:\" + base64data)\n","      resolve(base64data.toString())\n","\n","    });\n","\n","    }\n","    });\n","\n","    </script>\n","    \"\"\"\n","\n","    display(HTML(AUDIO_HTML))\n","    data = eval_js(\"data\")\n","    binary = b64decode(data.split(',')[1])\n","\n","    process = (ffmpeg\n","      .input('pipe:0')\n","      .output('pipe:1', format='wav')\n","      .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n","    )\n","    output, err = process.communicate(input=binary)\n","\n","    riff_chunk_size = len(output) - 8\n","    # Break up the chunk size into four bytes, held in b.\n","    q = riff_chunk_size\n","    b = []\n","    for i in range(4):\n","        q, r = divmod(q, 256)\n","        b.append(r)\n","\n","    # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n","    riff = output[:4] + bytes(b) + output[8:]\n","\n","    sr, audio = wav_read(io.BytesIO(riff))\n","\n","    return audio, sr\n","\n","\n","  # 음성을 받아서 텍스트로 처리하는 함수\n","  def whisper_model(audio):\n","\n","      # load audio and pad/trim it to fit 30 seconds\n","      audio = whisper.load_audio(audio)\n","      audio = whisper.pad_or_trim(audio)\n","\n","      # make log-Mel spectrogram and move to the same device as the model\n","      mel = whisper.log_mel_spectrogram(audio).to(model_whisper.device)\n","\n","      # detect the spoken language\n","      _, probs = model_whisper.detect_language(mel)\n","      # print(f\"Detected language: {max(probs, key=probs.get)}\")\n","\n","      # decode the audio\n","      options = whisper.DecodingOptions()\n","      result = whisper.decode(model_whisper, mel, options)\n","      return result.text\n","\n","\n","  def convert_wav_to_mp3(input_wav, output_mp3):\n","    subprocess.call(['ffmpeg', '-i', input_wav, output_mp3])\n","\n","\n","  def audio_to_tempfile(audio_data, sampling_rate):\n","    # 임시 WAV 파일에 오디오 데이터를 저장합니다.\n","    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n","        temp_filename = temp_file.name\n","        with wave.open(temp_filename, 'wb') as wave_file:\n","            wave_file.setnchannels(1)\n","            wave_file.setsampwidth(2)\n","            wave_file.setframerate(sampling_rate)\n","            wave_file.writeframes(audio_data)\n","\n","        # convert_wav_to_mp3 함수를 사용하여 WAV 파일을 MP3 파일로 변환합니다.\n","        temp_mp3_file = tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False)\n","        temp_mp3_file = temp_mp3_file.name\n","        convert_wav_to_mp3(temp_filename, temp_mp3_file)\n","\n","    return(temp_filename)\n","\n","\n","    #음성 인식\n","  audio_data, sampling_rate = get_audio()\n","\n","  audio_file_mp3 = audio_to_tempfile(audio_data, sampling_rate)\n","\n","  #받은 데이터\n","  result = whisper_model(audio_file_mp3)\n","\n","  noun_list = okt.nouns(result)\n","\n","  noun_result = stop_words_func(noun_list)\n","\n","  # noun_list = okt.nouns(result)\n","\n","  # noun_result = ' '.join(noun_list)\n","\n","  return result, noun_result"],"metadata":{"id":"LQB3HTwcpH6e","executionInfo":{"status":"ok","timestamp":1702133913544,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["from subprocess import run, PIPE\n","from google.colab import files\n","\n","def whisper_stt_read():\n","  def audio_to_tempfile(audio_data, sampling_rate):\n","      # 임시 WAV 파일에 오디오 데이터 저장\n","      with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as temp_file:\n","          temp_filename = temp_file.name\n","          with wave.open(temp_filename, 'wb') as wave_file:\n","              wave_file.setnchannels(1)\n","              wave_file.setsampwidth(2)\n","              wave_file.setframerate(sampling_rate)\n","              wave_file.writeframes(audio_data)\n","\n","      return temp_filename\n","\n","  def whisper_model(audio):\n","\n","      # load audio and pad/trim it to fit 30 seconds\n","      audio = whisper.load_audio(audio)\n","      audio = whisper.pad_or_trim(audio)\n","\n","      # make log-Mel spectrogram and move to the same device as the model\n","      mel = whisper.log_mel_spectrogram(audio).to(model_whisper.device)\n","\n","      # detect the spoken language\n","      _, probs = model_whisper.detect_language(mel)\n","      # print(f\"Detected language: {max(probs, key=probs.get)}\")\n","\n","      # decode the audio\n","      options = whisper.DecodingOptions()\n","      result = whisper.decode(model_whisper, mel, options)\n","      return result.text\n","\n","  def get_audio():\n","      # 파일 업로드 함수\n","      uploaded_files = files.upload()\n","      if not uploaded_files:\n","          print(\"파일이 업로드되지 않았습니다.\")\n","          return None\n","\n","      # 업로드한 파일 확인\n","      file_names = list(uploaded_files.keys())\n","      file_contents = uploaded_files[file_names[0]]\n","\n","      # 업로드한 파일을 ffmpeg를 사용하여 WAV로 변환\n","      process = run(['ffmpeg', '-i', '-', '-f', 'wav', '-'], input=file_contents, stdout=PIPE, stderr=PIPE)\n","      wav_data = process.stdout\n","\n","      sr, audio = wav_read(io.BytesIO(wav_data))\n","      return audio, sr\n","\n","  audio_data, sampling_rate = get_audio()\n","\n","  audio_file_mp3 = audio_to_tempfile(audio_data, sampling_rate)\n","\n","  #받은 데이터\n","  result = whisper_model(audio_file_mp3)\n","\n","\n","  noun_list = okt.nouns(result)\n","\n","  noun_result = stop_words_func(noun_list)\n","\n","  # noun_list = okt.nouns(result)\n","\n","  # noun_result = ' '.join(noun_list)\n","\n","\n","  return result, noun_result"],"metadata":{"id":"m-wESwwYsOsH","executionInfo":{"status":"ok","timestamp":1702133916475,"user_tz":-540,"elapsed":493,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["#법령-일상 연계"],"metadata":{"id":"Wh4EJ-cZ7Us_"}},{"cell_type":"code","source":["def legal_common_integration(noun_result):\n","\n","  term_indices = []\n","  term_legal = []\n","\n","  for keyword in noun_result:\n","    term_legal.append(keyword)\n","\n","    # indices_containing_word = df_common_term[df_common_term['term_name'].str.contains(keyword)].index\n","\n","    for index, row in df_common_term.iterrows():\n","        if row['term_name'] == keyword:\n","            term_indices.append(index)\n","    print(term_indices)\n","\n","    # 찾은 인덱스를 사용하여 'B' 열의 값을 출력\n","    for idx in term_indices:\n","      url_df = df_common_term.at[idx, 'term_link']\n","      url = 'https://www.law.go.kr' + url_df\n","\n","      print(url)\n","\n","      try:\n","          result = urlopen(url)\n","      except HTTPError as e:\n","          term_legal.append(df_common_term.at[idx, 'term_name'])\n","          # error_list.append([index, url])\n","          # print(\"HTTP error\")\n","          continue  # HTTP 오류가 발생하면 다음 반복으로 넘어감\n","\n","      house = BeautifulSoup(result, 'lxml-xml')\n","      te = house.find_all('연계용어')\n","\n","      check_num = 0\n","      for i in range(len(te)):\n","          legal_term = te[i].법령용어명.string.strip()\n","          term_legal.append(legal_term)\n","          check_num += 1\n","          if check_num ==2:\n","            break\n","\n","  return term_legal"],"metadata":{"id":"TcPqqnXJggnM","executionInfo":{"status":"ok","timestamp":1702133191621,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# transformer"],"metadata":{"id":"Kfll8-q4wNGw"}},{"cell_type":"code","source":["# 리스트 불러오기\n","with open(path + 'input_text_violent.json', 'r') as f:\n","    input_text_violent = json.load(f)\n","\n","with open(path + 'output_text_violent.json', 'r') as f:\n","    output_text_violent = json.load(f)\n","\n","# 서브워드텍스트인코더를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성\n","tokenizer_violent = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    input_text_violent + output_text_violent, target_vocab_size=2**13)\n","\n","# 시작 토큰과 종료 토큰에 대한 정수 부여.\n","START_TOKEN_violent, END_TOKEN_violent = [tokenizer_violent.vocab_size], [tokenizer_violent.vocab_size + 1]\n","\n","# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n","VOCAB_SIZE_violent = tokenizer_violent.vocab_size + 2\n","\n","# 리스트 불러오기\n","with open(path + 'input_text_scam.json', 'r') as f:\n","    input_text_scam = json.load(f)\n","\n","with open(path + 'output_text_scam.json', 'r') as f:\n","    output_text_scam = json.load(f)\n","\n","# 서브워드텍스트인코더를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성\n","tokenizer_scam = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n","    input_text_scam + output_text_scam, target_vocab_size=2**13)\n","\n","# 시작 토큰과 종료 토큰에 대한 정수 부여.\n","START_TOKEN_scam, END_TOKEN_scam = [tokenizer_scam.vocab_size], [tokenizer_scam.vocab_size + 1]\n","\n","# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n","VOCAB_SIZE_scam = tokenizer_scam.vocab_size + 2"],"metadata":{"id":"0t3RvaQz5G9v","executionInfo":{"status":"ok","timestamp":1702133226103,"user_tz":-540,"elapsed":34486,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["MAX_LENGTH = 500\n","\n","class PositionalEncoding(tf.keras.layers.Layer):\n","  def __init__(self, position, d_model):\n","    super(PositionalEncoding, self).__init__()\n","    self.pos_encoding = self.positional_encoding(position, d_model)\n","\n","  def get_angles(self, position, i, d_model):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position, d_model):\n","    angle_rads = self.get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n","        d_model=d_model)\n","\n","    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","\n","    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    angle_rads = np.zeros(angle_rads.shape)\n","    angle_rads[:, 0::2] = sines\n","    angle_rads[:, 1::2] = cosines\n","    pos_encoding = tf.constant(angle_rads)\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","\n","    # print(pos_encoding.shape)\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, inputs):\n","    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n","\n","  # def get_config(self):\n","  #   base_config = super().get_config()\n","  #   config = {\n","  #       \"position\": tf.keras.saving.serialize_keras_object(self.position),\n","  #   }\n","  #   return {**base_config, **config}\n","\n","  # def get_config(self):\n","\n","  #   config = super(PositionalEncoding, self).get_config()\n","  #   config.update({\n","  #       'position': self.position,\n","  #       'd_model': self.d_model,\n","\n","  #   })\n","  #   return config\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n","    super(MultiHeadAttention, self).__init__(name=name)\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","\n","    # d_model을 num_heads로 나눈 값.\n","    # 논문 기준 : 64\n","    self.depth = d_model // self.num_heads\n","\n","    # WQ, WK, WV에 해당하는 밀집층 정의\n","    self.query_dense = tf.keras.layers.Dense(units=d_model)\n","    self.key_dense = tf.keras.layers.Dense(units=d_model)\n","    self.value_dense = tf.keras.layers.Dense(units=d_model)\n","\n","    # WO에 해당하는 밀집층 정의\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  # num_heads 개수만큼 q, k, v를 split하는 함수\n","  def split_heads(self, inputs, batch_size):\n","    inputs = tf.reshape(\n","        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n","\n","  def call(self, inputs):\n","    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n","        'value'], inputs['mask']\n","    batch_size = tf.shape(query)[0]\n","\n","    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n","    # q : (batch_size, query의 문장 길이, d_model)\n","    # k : (batch_size, key의 문장 길이, d_model)\n","    # v : (batch_size, value의 문장 길이, d_model)\n","    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n","    query = self.query_dense(query)\n","    key = self.key_dense(key)\n","    value = self.value_dense(value)\n","\n","    # 2. 헤드 나누기\n","    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","    query = self.split_heads(query, batch_size)\n","    key = self.split_heads(key, batch_size)\n","    value = self.split_heads(value, batch_size)\n","\n","    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n","    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n","    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","    # 4. 헤드 연결(concatenate)하기\n","    # (batch_size, query의 문장 길이, d_model)\n","    concat_attention = tf.reshape(scaled_attention,\n","                                  (batch_size, -1, self.d_model))\n","\n","    # 5. WO에 해당하는 밀집층 지나기\n","    # (batch_size, query의 문장 길이, d_model)\n","    outputs = self.dense(concat_attention)\n","\n","    return outputs\n","\n","def scaled_dot_product_attention(query, key, value, mask):\n","  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n","  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n","  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n","\n","  # Q와 K의 곱. 어텐션 스코어 행렬.\n","  matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","  # 스케일링\n","  # dk의 루트값으로 나눠준다.\n","  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","  logits = matmul_qk / tf.math.sqrt(depth)\n","\n","  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n","  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n","  if mask is not None:\n","    logits += (mask * -1e9)\n","\n","  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n","  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n","  attention_weights = tf.nn.softmax(logits, axis=-1)\n","\n","  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n","  output = tf.matmul(attention_weights, value)\n","\n","  return output, attention_weights\n","\n","def create_padding_mask(x):\n","  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n","  # (batch_size, 1, 1, key의 문장 길이)\n","  return mask[:, tf.newaxis, tf.newaxis, :]\n","\n","def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n","  attention = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention\")({\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': padding_mask # 패딩 마스크 사용\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n","  attention = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(inputs + attention)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention + outputs)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","def encoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name=\"encoder\"):\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 인코더는 패딩 마스크 사용\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 인코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n","    )([outputs, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n","\n","# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n","def create_look_ahead_mask(x):\n","  seq_len = tf.shape(x)[1]\n","  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n","  return tf.maximum(look_ahead_mask, padding_mask)\n","\n","def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n","  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name=\"look_ahead_mask\")\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n","  attention1 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_1\")(inputs={\n","          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n","          'mask': look_ahead_mask # 룩어헤드 마스크\n","      })\n","\n","  # 잔차 연결과 층 정규화\n","  attention1 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention1 + inputs)\n","\n","  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n","  attention2 = MultiHeadAttention(\n","      d_model, num_heads, name=\"attention_2\")(inputs={\n","          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n","          'mask': padding_mask # 패딩 마스크\n","      })\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n","  attention2 = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(attention2 + attention1)\n","\n","  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n","  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n","  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n","\n","  # 드롭아웃 + 잔차 연결과 층 정규화\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n","  outputs = tf.keras.layers.LayerNormalization(\n","      epsilon=1e-6)(outputs + attention2)\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def decoder(vocab_size, num_layers, dff,\n","            d_model, num_heads, dropout,\n","            name='decoder'):\n","  inputs = tf.keras.Input(shape=(None,), name='inputs')\n","  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n","\n","  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n","  look_ahead_mask = tf.keras.Input(\n","      shape=(1, None, None), name='look_ahead_mask')\n","  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n","\n","  # 포지셔널 인코딩 + 드롭아웃\n","  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n","  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n","  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n","  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n","\n","  # 디코더를 num_layers개 쌓기\n","  for i in range(num_layers):\n","    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n","        dropout=dropout, name='decoder_layer_{}'.format(i),\n","    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n","\n","  return tf.keras.Model(\n","      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n","      outputs=outputs,\n","      name=name)\n","\n","def transformer(vocab_size, num_layers, dff,\n","                d_model, num_heads, dropout,\n","                name=\"transformer\"):\n","\n","  # 인코더의 입력\n","  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n","\n","  # 디코더의 입력\n","  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n","\n","  # 인코더의 패딩 마스크\n","  enc_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='enc_padding_mask')(inputs)\n","\n","  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n","  look_ahead_mask = tf.keras.layers.Lambda(\n","      create_look_ahead_mask, output_shape=(1, None, None),\n","      name='look_ahead_mask')(dec_inputs)\n","\n","  # 디코더의 패딩 마스크(두번째 서브층)\n","  dec_padding_mask = tf.keras.layers.Lambda(\n","      create_padding_mask, output_shape=(1, 1, None),\n","      name='dec_padding_mask')(inputs)\n","\n","  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n","  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n","\n","  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n","  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n","      d_model=d_model, num_heads=num_heads, dropout=dropout,\n","  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n","\n","  # 다음 단어 예측을 위한 출력층\n","  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n","\n","  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n","\n","def loss_function(y_true, y_pred):\n","  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True, reduction='none')(y_true, y_pred)\n","\n","  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","  loss = tf.multiply(loss, mask)\n","\n","  return tf.reduce_mean(loss)\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, tf.float32)\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps**-1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","  # def get_config(self):\n","  #       config = {\n","  #           'd_model':self.d_model,\n","  #           'warmup_steps':self.warmup_steps\n","\n","  #       }\n","  #       base_config = super(CustomSchedule, self).get_config()\n","  #       return dict(list(base_config.items()) + list(config.items()))\n","\n","  def get_config(self):\n","    config = {\n","    'd_model': self.d_model,\n","    'warmup_steps': self.warmup_steps,\n","\n","     }\n","    return config\n","\n","# def evaluate(model, sentence):\n","#   sentence = preprocess_sentence(sentence)\n","\n","#   sentence = tf.expand_dims(\n","#       START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n","\n","#   output = tf.expand_dims(START_TOKEN, 0)\n","\n","#   # 디코더의 예측 시작\n","#   for i in range(MAX_LENGTH):\n","#     predictions = model(inputs=[sentence, output], training=False)\n","\n","#     # 현재(마지막) 시점의 예측 단어를 받아온다.\n","#     predictions = predictions[:, -1:, :]\n","#     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","#     # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","#     if tf.equal(predicted_id, END_TOKEN[0]):\n","#       break\n","\n","#     # 마지막 시점의 예측 단어를 출력에 연결한다.\n","#     # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","#     output = tf.concat([output, predicted_id], axis=-1)\n","\n","#   return tf.squeeze(output, axis=0)\n","\n","\n","# def predict(model, sentence):\n","#   prediction = evaluate(model, sentence)\n","\n","#   predicted_sentence = tokenizer.decode(\n","#       [i for i in prediction if i < tokenizer.vocab_size])\n","\n","#   print('Input: {}'.format(sentence))\n","#   print('Output: {}'.format(predicted_sentence))\n","\n","#   return predicted_sentence\n","\n","def preprocess_sentence(sentence):\n","  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n","  sentence = sentence.strip()\n","  return sentence"],"metadata":{"id":"XQUAYe380IDq","executionInfo":{"status":"ok","timestamp":1702133226104,"user_tz":-540,"elapsed":17,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Hyper-parameters\n","NUM_LAYERS = 2 # 2\n","D_MODEL = 256 # 256\n","NUM_HEADS = 8\n","DFF = 512\n","DROPOUT = 0.1\n","\n","model_violent = transformer(\n","    vocab_size=VOCAB_SIZE_violent,\n","    num_layers=NUM_LAYERS,\n","    dff=DFF,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n","model_violent.load_weights(path + 'model_violent_weights').expect_partial()\n","\n","model_scam = transformer(\n","    vocab_size=VOCAB_SIZE_scam,\n","    num_layers=NUM_LAYERS,\n","    dff=DFF,\n","    d_model=D_MODEL,\n","    num_heads=NUM_HEADS,\n","    dropout=DROPOUT)\n","\n","model_scam.load_weights(path + 'model_scam_weights').expect_partial()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puK1MuLf3ais","executionInfo":{"status":"ok","timestamp":1702133237332,"user_tz":-540,"elapsed":11244,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"28e2c9fa-69f1-4b0f-86fc-a5155c8f8130"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x78337038a710>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def transformer_violent(term_list):\n","\n","  def evaluate(model, sentence):\n","    sentence = preprocess_sentence(sentence)\n","\n","    sentence = tf.expand_dims(\n","        START_TOKEN_violent + tokenizer_violent.encode(sentence) + END_TOKEN_violent, axis=0)\n","\n","    output = tf.expand_dims(START_TOKEN_violent, 0)\n","\n","    # 디코더의 예측 시작\n","    for i in range(MAX_LENGTH):\n","      predictions = model(inputs=[sentence, output], training=False)\n","\n","      # 현재(마지막) 시점의 예측 단어를 받아온다.\n","      predictions = predictions[:, -1:, :]\n","      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","      # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","      if tf.equal(predicted_id, END_TOKEN_violent[0]):\n","        break\n","\n","      # 마지막 시점의 예측 단어를 출력에 연결한다.\n","      # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","      output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0)\n","\n","  def predict(model, sentence):\n","    prediction = evaluate(model, sentence)\n","\n","    predicted_sentence = tokenizer_violent.decode(\n","        [i for i in prediction if i < tokenizer_violent.vocab_size])\n","\n","    print('Input: {}'.format(sentence))\n","    print('Output: {}'.format(predicted_sentence))\n","\n","    return predicted_sentence\n","\n","  result_term = ' '.join(term_list)\n","\n","  output = predict(model_violent, result_term)\n","\n","  return output"],"metadata":{"id":"E9wLHmcxwP62","executionInfo":{"status":"ok","timestamp":1702133312386,"user_tz":-540,"elapsed":303,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def transformer_scam(term_list):\n","\n","  def evaluate(model, sentence):\n","    sentence = preprocess_sentence(sentence)\n","\n","    sentence = tf.expand_dims(\n","        START_TOKEN_scam + tokenizer_scam.encode(sentence) + END_TOKEN_scam, axis=0)\n","\n","    output = tf.expand_dims(START_TOKEN_scam, 0)\n","\n","    # 디코더의 예측 시작\n","    for i in range(MAX_LENGTH):\n","      predictions = model(inputs=[sentence, output], training=False)\n","\n","      # 현재(마지막) 시점의 예측 단어를 받아온다.\n","      predictions = predictions[:, -1:, :]\n","      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","      # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n","      if tf.equal(predicted_id, END_TOKEN_scam[0]):\n","        break\n","\n","      # 마지막 시점의 예측 단어를 출력에 연결한다.\n","      # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n","      output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0)\n","\n","  def predict(model, sentence):\n","    prediction = evaluate(model, sentence)\n","\n","    predicted_sentence = tokenizer_scam.decode(\n","        [i for i in prediction if i < tokenizer_scam.vocab_size])\n","\n","    print('Input: {}'.format(sentence))\n","    print('Output: {}'.format(predicted_sentence))\n","\n","    return predicted_sentence\n","\n","  result_term = ' '.join(term_list)\n","\n","  output = predict(model_scam, result_term)\n","\n","  return output"],"metadata":{"id":"Zh56ehpvwP39","executionInfo":{"status":"ok","timestamp":1702133312656,"user_tz":-540,"elapsed":2,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# 실행"],"metadata":{"id":"tfDFY_4zk3WW"}},{"cell_type":"code","source":["# 폭행1\n","print(\"다음 보기 중 문의하려는 키워드의 번호를 선택해주세요.\")\n","\n","print(\"1. 폭행\")\n","print(\"2. 사기\")\n","print(\"\\n\")\n","\n","user_input = input(\"번호: \")\n","\n","# result, noun_result = whisper_stt()\n","result, noun_result = whisper_stt_read()\n","\n","print(\"말씀하신 문장은 다음과 같습니다\")\n","\n","print('\"' + result + '\"')\n","\n","if user_input == \"1\":\n","  output = transformer_violent(noun_result)\n","elif user_input ==\"2\":\n","  output = transformer_scam(noun_result)"],"metadata":{"id":"z-6AY7F0tBXX","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1702134154119,"user_tz":-540,"elapsed":13352,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"f36b52ef-b424-4bc5-92af-db62d1ba256e"},"execution_count":34,"outputs":[{"name":"stdout","output_type":"stream","text":["다음 보기 중 문의하려는 키워드의 번호를 선택해주세요.\n","1. 폭행\n","2. 사기\n","\n","\n","번호: 1\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1b3ba011-b305-4a78-b1e6-df55f978aecf\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1b3ba011-b305-4a78-b1e6-df55f978aecf\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 녹음_폭행1.wav to 녹음_폭행1 (3).wav\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-28-30b9ab9c941c>:50: WavFileWarning: Reached EOF prematurely; finished at 352369 bytes, expected 4294967303 bytes from header.\n","  sr, audio = wav_read(io.BytesIO(wav_data))\n"]},{"output_type":"stream","name":"stdout","text":["말씀하신 문장은 다음과 같습니다\n","\"성추행과 강간을 당했습니다.\"\n","Input: 성추행 강간\n","Output: 성폭력범죄의처벌등에관한특례법위반\n"]}]},{"cell_type":"code","source":["# 폭행2\n","print(\"다음 보기 중 문의하려는 키워드의 번호를 선택해주세요.\")\n","\n","print(\"1. 폭행\")\n","print(\"2. 사기\")\n","print(\"\\n\")\n","\n","user_input = input(\"번호: \")\n","\n","# result, noun_result = whisper_stt()\n","result, noun_result = whisper_stt_read()\n","\n","print(\"말씀하신 문장은 다음과 같습니다\")\n","\n","print('\"' + result + '\"')\n","\n","if user_input == \"1\":\n","  output = transformer_violent(noun_result)\n","elif user_input ==\"2\":\n","  output = transformer_scam(noun_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"8vKhDIF06XTe","executionInfo":{"status":"ok","timestamp":1702134176460,"user_tz":-540,"elapsed":18919,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"53371027-198d-4bae-9c4b-6f650119c97f"},"execution_count":35,"outputs":[{"name":"stdout","output_type":"stream","text":["다음 보기 중 문의하려는 키워드의 번호를 선택해주세요.\n","1. 폭행\n","2. 사기\n","\n","\n","번호: 1\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-98cd32cb-d489-484d-94b0-47dd955e9f0f\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-98cd32cb-d489-484d-94b0-47dd955e9f0f\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 녹음_폭행2.wav to 녹음_폭행2 (1).wav\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-28-30b9ab9c941c>:50: WavFileWarning: Reached EOF prematurely; finished at 604273 bytes, expected 4294967303 bytes from header.\n","  sr, audio = wav_read(io.BytesIO(wav_data))\n"]},{"output_type":"stream","name":"stdout","text":["말씀하신 문장은 다음과 같습니다\n","\"술집에서 술을 마시다가 쌍방폭행으로 싸움이 일어났습니다.\"\n","Input: 술집 술 쌍방 폭행 싸움\n","Output: 폭력행위등처벌에관한법률위반\n"]}]},{"cell_type":"code","source":["# 사기1\n","print(\"다음 보기 중 문의하려는 키워드의 번호를 선택해주세요.\")\n","\n","print(\"1. 폭행\")\n","print(\"2. 사기\")\n","print(\"\\n\")\n","\n","user_input = input(\"번호: \")\n","\n","# result, noun_result = whisper_stt()\n","result, noun_result = whisper_stt_read()\n","\n","print(\"말씀하신 문장은 다음과 같습니다\")\n","\n","print('\"' + result + '\"')\n","\n","if user_input == \"1\":\n","  output = transformer_violent(noun_result)\n","elif user_input ==\"2\":\n","  output = transformer_scam(noun_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"vNK5XHPKGg1F","executionInfo":{"status":"ok","timestamp":1702134200678,"user_tz":-540,"elapsed":15586,"user":{"displayName":"‍이승건[학생](공과대학 산업경영공학과)","userId":"07134745930762313936"}},"outputId":"80448955-f4ae-4db1-ba8d-485a09663e6f"},"execution_count":36,"outputs":[{"name":"stdout","output_type":"stream","text":["다음 보기 중 문의하려는 키워드의 번호를 선택해주세요.\n","1. 폭행\n","2. 사기\n","\n","\n","번호: 2\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1c1d2a79-66fc-43b6-a80a-fd1ea08d1612\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1c1d2a79-66fc-43b6-a80a-fd1ea08d1612\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 녹음_사기1.wav to 녹음_사기1 (4).wav\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-28-30b9ab9c941c>:50: WavFileWarning: Reached EOF prematurely; finished at 579693 bytes, expected 4294967303 bytes from header.\n","  sr, audio = wav_read(io.BytesIO(wav_data))\n"]},{"output_type":"stream","name":"stdout","text":["말씀하신 문장은 다음과 같습니다\n","\"부동산 증서를 양도하여 계약하였는데 사기를 당한 것 같아요\"\n","Input: 부동산 증서 양도 계약 사기\n","Output: 사문서위조 사문서위조행사 공정증서원본불실기재 동행사 사기 횡령\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DolGchiFGnn4"},"execution_count":null,"outputs":[]}]}